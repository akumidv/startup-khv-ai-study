{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "colab": {
   "name": "01 Q-Learning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqOA7i-I91HR"
   },
   "source": [
    "## Q-Learning: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiqbkBKf91HV"
   },
   "source": [
    "![](https://i.ibb.co/c8LXj7X/Capture.png)\n",
    "\n",
    "<center>Окружение</center>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dJTQwCtL91HW"
   },
   "source": [
    "import numpy as np"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kav7oSfj91HX"
   },
   "source": [
    "gamma = 0.75 # Дискаунт\n",
    "alpha = 0.9 # Коэффициент обучения\n",
    "\n",
    "# Кодирование состояний \n",
    "location_to_state = {'L1' : 0, 'L2' : 1, 'L3' : 2, 'L4' : 3,\n",
    "                    'L5' : 4, 'L6' : 5, 'L7' : 6, 'L8' : 7, 'L9' : 8}\n",
    "state_to_location = dict((state,location) for location,state in location_to_state.items()) # Словарь кода и состояния\n",
    "# Коды действий\n",
    "actions = [0,1,2,3,4,5,6,7,8]"
   ],
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPS_35_W91HY"
   },
   "source": [
    "![](https://i.ibb.co/k4kgnQS/Capture.png)\n",
    "\n",
    "<center>Таблица награды</center>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S3w6PcfC91HZ"
   },
   "source": [
    "# Награда\n",
    "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
    "                    [1,0,1,0,0,0,0,0,0],\n",
    "                    [0,1,0,0,0,1,0,0,0],\n",
    "                    [0,0,0,0,0,0,1,0,0],\n",
    "                    [0,1,0,0,0,0,0,1,0],\n",
    "                    [0,0,1,0,0,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,0,1,0],\n",
    "                    [0,0,0,0,1,0,1,0,1],\n",
    "                    [0,0,0,0,0,0,0,1,0]])"
   ],
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0DLjujbFdaO",
    "outputId": "7e62f7ec-7a6d-4312-97ab-24d601d103ec"
   },
   "source": [
    "rewards.shape[1]"
   ],
   "execution_count": 53,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-g607KF91Hb"
   },
   "source": [
    "The following function is going to take two arguments: \n",
    "\n",
    "- starting location in the warehouse and \n",
    "- end location in the warehouse respectively \n",
    "\n",
    "It will return the optimal route for reaching the end location from the starting location in the form of an ordered list (containing the letters)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8pryBj_kGl2c"
   },
   "source": [
    "# -----------Q-Learning algorithm-----------\n",
    "def q_learn(end_location):\n",
    "  Q = np.array(np.zeros(rewards.shape)) # Инициализация массива наград\n",
    "  ending_state = location_to_state[end_location]\n",
    "  rewards_new = np.copy(rewards)\n",
    "  rewards_new[ending_state,ending_state] = 999 # Увеличиваем награду за конечное состояние\n",
    "  dimension = rewards.shape[1]\n",
    "  for i in range(1000):\n",
    "    current_state = np.random.randint(0, dimension) # Случайное состояние\n",
    "    playable_actions = [] # Действия для перехода к соседней локации\n",
    "    for j in range(dimension): # Определяем возможные действия с текущего состояния\n",
    "        if rewards_new[current_state, j] > 0:\n",
    "            playable_actions.append(j)\n",
    "    next_state = np.random.choice(playable_actions) # Случайно выбираем переход\n",
    "    # Определяем временную разницу\n",
    "    TD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]\n",
    "    # Обновляем значение качества с коэффициентом обучения по уравнению Беллмана\n",
    "    Q[current_state,next_state] += alpha * TD\n",
    "  print(Q)\n",
    "  return Q"
   ],
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CyXQPDaH91Hb"
   },
   "source": [
    "def get_optimal_route(start_location, end_location):\n",
    "    ending_state = location_to_state[end_location]\n",
    "\n",
    "    Q = q_learn(end_location) # Формируем матрицу качества принятия решений для состояний\n",
    "    route = [start_location] # Маршрут начинается с текущей позиции\n",
    "    next_location = start_location\n",
    "    \n",
    "    while next_location != end_location:\n",
    "        # Fetch the starting state\n",
    "        starting_state = location_to_state[start_location] # Получаем начальное состояние\n",
    "        next_state = np.argmax(Q[starting_state,]) # Ищем переход в максимальное по качеству состояние\n",
    "        next_location = state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        start_location = next_location\n",
    "    return route"
   ],
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlOQisqNIKOi"
   },
   "source": [
    "![](https://i.ibb.co/c8LXj7X/Capture.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2o661cTGh_j",
    "outputId": "63eb0abc-9b67-4245-f92e-a09c1a2aff2d"
   },
   "source": [
    "route = get_optimal_route('L9', 'L1')"
   ],
   "execution_count": 62,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[3996. 2249.    0.    0.    0.    0.    0.    0.    0.]\n",
      " [2998.    0. 1688.    0.    0.    0.    0.    0.    0.]\n",
      " [   0. 2249.    0.    0.    0. 1267.    0.    0.    0.]\n",
      " [   0.    0.    0.    0.    0.    0.  951.    0.    0.]\n",
      " [   0. 2249.    0.    0.    0.    0.    0. 1267.    0.]\n",
      " [   0.    0. 1688.    0.    0.    0.    0.    0.    0.]\n",
      " [   0.    0.    0.  714.    0.    0.    0. 1267.    0.]\n",
      " [   0.    0.    0.    0. 1688.    0.  951.    0.  951.]\n",
      " [   0.    0.    0.    0.    0.    0.    0. 1267.    0.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gureNr-mgTXd",
    "outputId": "d4eaba4b-907f-405b-d5f6-77fd4c50b91a"
   },
   "source": [
    "route"
   ],
   "execution_count": 63,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['L9', 'L8', 'L5', 'L2', 'L1']"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ]
  }
 ]
}