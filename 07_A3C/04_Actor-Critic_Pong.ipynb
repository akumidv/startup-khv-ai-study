{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\workspace\\trading\\study\\.env\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/10000, score: -21.0, average: -21.00 SAVING\n",
      "episode: 1/10000, score: -19.0, average: -20.00 SAVING\n",
      "episode: 2/10000, score: -21.0, average: -20.33 \n",
      "episode: 3/10000, score: -19.0, average: -20.00 SAVING\n",
      "episode: 4/10000, score: -20.0, average: -20.00 SAVING\n",
      "episode: 5/10000, score: -20.0, average: -20.00 SAVING\n",
      "episode: 6/10000, score: -21.0, average: -20.14 \n",
      "episode: 7/10000, score: -19.0, average: -20.00 SAVING\n",
      "episode: 8/10000, score: -20.0, average: -20.00 SAVING\n",
      "episode: 9/10000, score: -19.0, average: -19.90 SAVING\n",
      "episode: 10/10000, score: -21.0, average: -20.00 \n",
      "episode: 11/10000, score: -21.0, average: -20.08 \n",
      "episode: 12/10000, score: -20.0, average: -20.08 \n",
      "episode: 13/10000, score: -19.0, average: -20.00 \n",
      "episode: 14/10000, score: -18.0, average: -19.87 SAVING\n",
      "episode: 15/10000, score: -21.0, average: -19.94 \n",
      "episode: 16/10000, score: -21.0, average: -20.00 \n",
      "episode: 17/10000, score: -21.0, average: -20.06 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_27672/160767457.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    241\u001B[0m     \u001B[0menv_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'Pong-v0'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    242\u001B[0m     \u001B[0magent\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mA2CAgent\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0menv_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 243\u001B[1;33m     \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    244\u001B[0m     \u001B[1;31m#agent.test('Pong-v0_A2C_2.5e-05_Actor.h5', '')\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_27672/160767457.py\u001B[0m in \u001B[0;36mrun\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    199\u001B[0m                 \u001B[1;31m#self.env.render()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m                 \u001B[1;31m# Actor picks an action\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m                 \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mact\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    202\u001B[0m                 \u001B[1;31m# Retrieve new state, reward, and whether the state is terminal\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m                 \u001B[0mnext_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_27672/160767457.py\u001B[0m in \u001B[0;36mact\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     79\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mact\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m         \u001B[1;31m# Use the network to predict the next action to take, using the model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 81\u001B[1;33m         \u001B[0mprediction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mActor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     82\u001B[0m         \u001B[0maction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchoice\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprediction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0maction\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\workspace\\trading\\study\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     63\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 64\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     65\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=broad-except\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\workspace\\trading\\study\\.env\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1783\u001B[0m       \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_predict_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1784\u001B[0m       \u001B[0mbatch_outputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1785\u001B[1;33m       \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0miterator\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0menumerate_epochs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Single epoch.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1786\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcatch_stop_iteration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1787\u001B[0m           \u001B[1;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msteps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\workspace\\trading\\study\\.env\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001B[0m in \u001B[0;36menumerate_epochs\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1193\u001B[0m     \u001B[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1194\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_truncate_execution_to_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1195\u001B[1;33m       \u001B[0mdata_iterator\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0miter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1196\u001B[0m       \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_initial_epoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1197\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_insufficient_data\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Set by `catch_stop_iteration`.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\workspace\\trading\\study\\.env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    488\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minside_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    489\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolocate_with\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_variant_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 490\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0miterator_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mOwnedIterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    491\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    492\u001B[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001B[1;32md:\\workspace\\trading\\study\\.env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, dataset, components, element_spec)\u001B[0m\n\u001B[0;32m    724\u001B[0m             \u001B[1;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    725\u001B[0m             \"not be specified.\")\n\u001B[1;32m--> 726\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    727\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    728\u001B[0m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_next_call_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\workspace\\trading\\study\\.env\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001B[0m in \u001B[0;36m_create_iterator\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    749\u001B[0m               \u001B[0moutput_types\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_flat_output_types\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    750\u001B[0m               output_shapes=self._flat_output_shapes))\n\u001B[1;32m--> 751\u001B[1;33m       \u001B[0mgen_dataset_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_iterator\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mds_variant\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_iterator_resource\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    752\u001B[0m       \u001B[1;31m# Delete the resource when this object is deleted\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    753\u001B[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001B[1;32md:\\workspace\\trading\\study\\.env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001B[0m in \u001B[0;36mmake_iterator\u001B[1;34m(dataset, iterator, name)\u001B[0m\n\u001B[0;32m   3234\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3235\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3236\u001B[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[0;32m   3237\u001B[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001B[0;32m   3238\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1296x648 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://pylessons.com/A2C-reinforcement-learning/\n",
    "# Tutorial written for - Tensorflow 1.15, Keras 2.2.4\n",
    "\n",
    "import os\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "\n",
    "\n",
    "def OurModel(input_shape, action_space, lr):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    #X = Conv2D(32, 8, strides=(4, 4),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\", input_shape=input_shape)(X_input)\n",
    "    #X = Conv2D(64, 4, strides=(2, 2),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\")(X)\n",
    "    #X = Conv2D(64, 3, strides=(1, 1),padding=\"valid\", activation=\"elu\", data_format=\"channels_first\")(X)\n",
    "    X = Flatten(input_shape=input_shape)(X_input)\n",
    "\n",
    "    X = Dense(512, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    #X = Dense(256, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "    #X = Dense(64, activation=\"elu\", kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    action = Dense(action_space, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "    value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "\n",
    "    Actor = Model(inputs = X_input, outputs = action)\n",
    "    Actor.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=lr))\n",
    "\n",
    "    Critic = Model(inputs = X_input, outputs = value)\n",
    "    Critic.compile(loss='mse', optimizer=RMSprop(lr=lr))\n",
    "\n",
    "    return Actor, Critic\n",
    "\n",
    "class A2CAgent:\n",
    "    # Actor-Critic Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.EPISODES, self.max_average = 10000, -21.0 # specific for pong\n",
    "        self.lr = 0.000025\n",
    "\n",
    "        self.ROWS = 80\n",
    "        self.COLS = 80\n",
    "        self.REM_STEP = 4\n",
    "\n",
    "        # Instantiate games and plot memory\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        self.scores, self.episodes, self.average = [], [], []\n",
    "\n",
    "        self.Save_Path = 'models'\n",
    "        self.state_size = (self.REM_STEP, self.ROWS, self.COLS)\n",
    "        self.image_memory = np.zeros(self.state_size)\n",
    "\n",
    "        if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n",
    "        self.path = '{}_A2C_{}'.format(self.env_name, self.lr)\n",
    "        self.Model_name = os.path.join(self.Save_Path, self.path)\n",
    "\n",
    "        # Create Actor-Critic network model\n",
    "        self.Actor, self.Critic = OurModel(input_shape=self.state_size, action_space = self.action_size, lr=self.lr)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward):\n",
    "        # store episode actions to memory\n",
    "        self.states.append(state)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(state)[0]\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        return action\n",
    "\n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            if reward[i] != 0: # reset the sum, since this was a game boundary (pong specific!)\n",
    "                running_add = 0\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= np.std(discounted_r) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(self.states)\n",
    "        actions = np.vstack(self.actions)\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_r = self.discount_rewards(self.rewards)\n",
    "\n",
    "        # Get Critic network predictions\n",
    "        values = self.Critic.predict(states)[:, 0]\n",
    "        # Compute advantages\n",
    "        advantages = discounted_r - values\n",
    "        # training Actor and Critic networks\n",
    "        self.Actor.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
    "        self.Critic.fit(states, discounted_r, epochs=1, verbose=0)\n",
    "        # reset training memory\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "    def load(self, Actor_name, Critic_name):\n",
    "        self.Actor = load_model(Actor_name, compile=False)\n",
    "        #self.Critic = load_model(Critic_name, compile=False)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.save(self.Model_name + '_Actor.h5')\n",
    "        #self.Critic.save(self.Model_name + '_Critic.h5')\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores.append(score)\n",
    "        self.episodes.append(episode)\n",
    "        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes, self.scores, 'b')\n",
    "            pylab.plot(self.episodes, self.average, 'r')\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.savefig(self.path+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "        return self.average[-1]\n",
    "\n",
    "    def imshow(self, image, rem_step=0):\n",
    "        cv2.imshow(self.Model_name+str(rem_step), image[rem_step,...])\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    def GetImage(self, frame):\n",
    "        # croping frame to 80x80 size\n",
    "        frame_cropped = frame[35:195:2, ::2,:]\n",
    "        if frame_cropped.shape[0] != self.COLS or frame_cropped.shape[1] != self.ROWS:\n",
    "            # OpenCV resize function\n",
    "            frame_cropped = cv2.resize(frame, (self.COLS, self.ROWS), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        # converting to RGB (numpy way)\n",
    "        frame_rgb = 0.299*frame_cropped[:,:,0] + 0.587*frame_cropped[:,:,1] + 0.114*frame_cropped[:,:,2]\n",
    "\n",
    "        # convert everything to black and white (agent will train faster)\n",
    "        frame_rgb[frame_rgb < 100] = 0\n",
    "        frame_rgb[frame_rgb >= 100] = 255\n",
    "        # converting to RGB (OpenCV way)\n",
    "        #frame_rgb = cv2.cvtColor(frame_cropped, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # dividing by 255 we expresses value to 0-1 representation\n",
    "        new_frame = np.array(frame_rgb).astype(np.float32) / 255.0\n",
    "\n",
    "        # push our data by 1 frame, similar as deq() function work\n",
    "        self.image_memory = np.roll(self.image_memory, 1, axis = 0)\n",
    "\n",
    "        # inserting new frame to free space\n",
    "        self.image_memory[0,:,:] = new_frame\n",
    "\n",
    "        # show image frame\n",
    "        #self.imshow(self.image_memory,0)\n",
    "        #self.imshow(self.image_memory,1)\n",
    "        #self.imshow(self.image_memory,2)\n",
    "        #self.imshow(self.image_memory,3)\n",
    "\n",
    "        return np.expand_dims(self.image_memory, axis=0)\n",
    "\n",
    "    def reset(self):\n",
    "        frame = self.env.reset()\n",
    "        for i in range(self.REM_STEP):\n",
    "            state = self.GetImage(frame)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        next_state = self.GetImage(next_state)\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def run(self):\n",
    "        for e in range(self.EPISODES):\n",
    "            state = self.reset()\n",
    "            done, score, SAVING = False, 0, ''\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                action = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                self.remember(state, action, reward)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    average = self.PlotModel(score, e)\n",
    "                    # saving best models\n",
    "                    if average >= self.max_average:\n",
    "                        self.max_average = average\n",
    "                        self.save()\n",
    "                        SAVING = \"SAVING\"\n",
    "                    else:\n",
    "                        SAVING = \"\"\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(e, self.EPISODES, score, average, SAVING))\n",
    "\n",
    "                    self.replay()\n",
    "        # close environemnt when finish training\n",
    "        self.env.close()\n",
    "\n",
    "    def test(self, Actor_name, Critic_name):\n",
    "        self.load(Actor_name, Critic_name)\n",
    "        for e in range(100):\n",
    "            state = self.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                action = np.argmax(self.Actor.predict(state))\n",
    "                state, reward, done, _ = self.step(action)\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, score))\n",
    "                    break\n",
    "        self.env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #env_name = 'PongDeterministic-v4'\n",
    "    env_name = 'Pong-v0'\n",
    "    agent = A2CAgent(env_name)\n",
    "    agent.run()\n",
    "    #agent.test('Pong-v0_A2C_2.5e-05_Actor.h5', '')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}