{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1_rule_and_sentiment_study.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PldDQYcwQ3Jc"
      },
      "source": [
        "Описание\n",
        "Исходный репозиторий: https://github.com/srbhklkrn/depression-therapist-chatbot\n",
        "\n",
        "Блок обучения модели https://github.com/srbhklkrn/depression-therapist-chatbot/blob/master/model/sent_model_vocab.py\n",
        "Исходный файл\n",
        "\n",
        "Логика работы:\n",
        "\n",
        "Особенности\n",
        "* в репозитории есть ссылка на готовую модель и код для обучения модели, в репозитории пустышки с названием хеш суммой\n",
        "* в репозитории нет ссылки на датасет, но указано, что кагл твиттеры. Вместо них пустышки с названием хеш суммой. В кагле есть основной датасет https://www.kaggle.com/c/tweet-sentiment-extraction\n",
        "* обучение на датасете твиттеров, которые разбиваются на набор слов не более 20ти символов длиной, чистятся от хештегов и авторов, ссылок и слов с xxx. \n",
        "* разбитые отрезки токенизируются и чистятся от слов короче 2х символов\n",
        "* словарь слов сохраняется в отдельный файл\n",
        "* модель обучается на сети с одним слоем из 128 нейронов. Точность определения тональности до 82%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3dF6Ew1REjn"
      },
      "source": [
        "Предварительные действия\n",
        "\n",
        "\n",
        "Ограничения и особенности модели:\n",
        "* для обработки текста использует `gensim` https://radimrehurek.com/gensim/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FSI-FQ6FKSX"
      },
      "source": [
        "# Зависимости\n",
        "import os\n",
        "import codecs\n",
        "import sys\n",
        "import numpy as np\n",
        "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, stem_text\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.regularizers import l2\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import time\n",
        "import csv\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqSjpAT-HM3-",
        "outputId": "1fbed55c-2983-41c4-9067-44db85aa9515"
      },
      "source": [
        "# Обеспечиваем подгрузку данных и их хранение в каталоге ноутубка MyDrive/chats_emotions_and_voises/chat04_depression-therapist-chatbot\n",
        "BASE_PATH='/content/gdrive/MyDrive/chats_emotions_and_voises/devdv_ALL/1_rule'\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "if not os.path.exists(BASE_PATH):\n",
        "    raise ValueError('Нет папки для хранения данных', BASE_PATH)\n",
        "%cd $BASE_PATH\n",
        "if not os.path.exists('Tensorboard'): \n",
        "  os.makedirs('Tensorboard')\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/chats_emotions_and_voises/chat04_depression-therapist-chatbot\n",
            "1model_nn.hdf5      data_tweets.csv                  \u001b[0m\u001b[01;34mTensorboard\u001b[0m/\n",
            "chat04_main.ipynb   model_nn.hdf5                    \u001b[01;34mtweet_data\u001b[0m/\n",
            "chat04_study.ipynb  model_nn_ORIGINAL.h5             vocab_sentiment\n",
            "data_labels.csv     sent_model_vocab_model-ORIGINAL  vocab_sentiment_ORIGINAL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L-J4B8WGmoS"
      },
      "source": [
        "# Всопомогательные функции\n",
        "\n",
        "# Функци анализа датасета твиттеров и извлечения из них твитов и меток тональности\n",
        "def export(type_data='train'):\n",
        "    print(\"Extracting data...\")\n",
        "    if type_data.lower() == 'train':\n",
        "        filename = 'train.csv'\n",
        "    elif type_data.lower() == 'test':\n",
        "        filename = 'test.csv'\n",
        "    data_file = codecs.open('tweet_data/' + filename, encoding='ISO-8859-1')\n",
        "    data = []\n",
        "    for tweet in data_file.read().split('\\n')[:-1]:\n",
        "        data.append([string for string in tweet.split('\"') if string not in [\n",
        "                    '', ',']])\n",
        "    data_file.close()\n",
        "    print(data[:3])\n",
        "    labels = [(float(tweet[0]) / 4.0) for tweet in data]\n",
        "    tweets = [tweet[-1] for tweet in data]\n",
        "    print(f'First 2 tweets fron {len(tweets)}\\n', tweets[:2], labels[:2])\n",
        "\n",
        "    print(\"Обработка данных (токензация и чистка от незначимых слов)...\")\n",
        "    for i, tweet in enumerate(tweets):\n",
        "        # Чистка от пустых слов, слов начинающихся с @ и # (аккаунты и хештеги), ссылок\n",
        "        new_tweet = ' '.join([word for word in tweet.split(' ') if len(word)\\\n",
        "                            > 0 and word[0] not in ['@', '#'] and 'http' not\\\n",
        "                            in word]).strip()\n",
        "        # Преобразование слов и чистка от заканчивающихся на xxx, а также без слов not и notxxx\n",
        "        pro_tweet = [word[:-3] if word[-3:] == 'xxx' else word for word in\n",
        "                    preprocess_string(new_tweet.replace('not', 'notxxx'))]\n",
        "        #pro_tweet = preprocess_string(new_tweet)\n",
        "        # Результат\n",
        "        # new_tweet: Yup!! saw the entire match....reached office late..\n",
        "        # pro_tweet:  ['yup', 'saw', 'entir', 'match', 'reach', 'offic', 'late']\n",
        "        if len(pro_tweet) < 2:\n",
        "            tweets[i] = strip_punctuation(stem_text(new_tweet.lower())).\\\n",
        "                        strip().split()\n",
        "        else:\n",
        "            tweets[i] = pro_tweet\n",
        "        # sys.stdout.write(\"\\r%d tweet(s) pre-processed out of %d\\r\" % (i + 1, len(tweets)))\n",
        "        # sys.stdout.flush()\n",
        "        if (i + 1) % 100000 == 0:\n",
        "            print(f\"{i+1:,} tweet(s) pre-processed out of {len(tweets):,}\")\n",
        "\n",
        "    print(\"\\nЧистка данных (от слов короче 2х символов)...\")\n",
        "    backup_tweets = np.array(tweets, dtype=object)\n",
        "    backup_labels = np.array(labels, dtype=object)\n",
        "    tweets = []\n",
        "    labels = []\n",
        "    for i, tweet in enumerate(backup_tweets):\n",
        "        if len(tweet) >= 2:\n",
        "            tweets.append(tweet)\n",
        "            labels.append(backup_labels[i])\n",
        "    del backup_tweets\n",
        "    del backup_labels\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    data = list(zip(tweets, labels))\n",
        "    np.random.shuffle(data)\n",
        "    tweets, labels = zip(*data)\n",
        "\n",
        "    return (tweets, labels)\n",
        "\n",
        "# Функция создания словаря\n",
        "def create_vocab(tweets):\n",
        "    print(\"Building vocabulary...\")\n",
        "    vocab = Dictionary()    \n",
        "    vocab.add_documents(tweets)\n",
        "    vocab.save('vocab_sentiment')\n",
        "    return vocab\n",
        "\n",
        "# Получение готового словаря или создание нового\n",
        "def get_vocab(tweets=None):\n",
        "    if 'vocab_sentiment' in os.listdir('.'):\n",
        "        if not tweets:\n",
        "            print(\"Loading vocabulary...\")\n",
        "            vocab = Dictionary.load('vocab_sentiment')\n",
        "            print(\"Loaded vocabulary\")\n",
        "            return vocab\n",
        "        # response = raw_input('Vocabulary found. Do you want to load it? (Y/n): ')\n",
        "        response = input('Vocabulary found. Do you want to load it? (Y/n):')\n",
        "        if response.lower() in ['n', 'no', 'nah', 'nono', 'nahi', 'nein']:\n",
        "            if not tweets:\n",
        "                tweets, labels = export()\n",
        "                del labels\n",
        "            return create_vocab(tweets)\n",
        "        else:\n",
        "            print(\"Loading vocabulary...\")\n",
        "            vocab = Dictionary.load('vocab_sentiment')\n",
        "            print(\"Loaded vocabulary\")\n",
        "            return vocab\n",
        "    else:\n",
        "        if not tweets:\n",
        "            tweets, labels = export()\n",
        "            del labels\n",
        "        return create_vocab(tweets)\n",
        "\n",
        "# Инициализация словаря\n",
        "def init_with_vocab(tweets=None, labels=None, vocab=None, type_data='train'):\n",
        "    if not tweets and not labels:\n",
        "        if type_data=='train':\n",
        "            if 'data_tweets.csv' in os.listdir('.') and 'data_labels.csv' in os.listdir('.'):\n",
        "                with open('data_tweets.csv', newline='') as f:\n",
        "                    reader = csv.reader(f)\n",
        "                    tweets = list(reader)\n",
        "                with open('data_labels.csv', newline='') as f:\n",
        "                    reader = csv.reader(f)\n",
        "                    labels = list(reader)\n",
        "                print(tweets[:2], labels[:2])\n",
        "            else:\n",
        "                tweets, labels = export(type_data)\n",
        "                with open('data_tweets.csv', 'w', newline='') as csvfile:\n",
        "                    writer= csv.writer(csvfile)\n",
        "                    writer.writerow(tweets)\n",
        "                with open('data_labels.csv', 'w', newline='') as csvfile:\n",
        "                    writer= csv.writer(csvfile)\n",
        "                    writer.writerow(labels)\n",
        "        else:\n",
        "            tweets, labels = export(type_data)\n",
        "    elif tweets and labels:\n",
        "        pass\n",
        "    else:\n",
        "        print(\"One of tweets or labels given, but not the other\")\n",
        "        return\n",
        "    if not vocab and type_data == 'train':\n",
        "        vocab = get_vocab(tweets)\n",
        "    elif not vocab:\n",
        "        vocab = get_vocab()\n",
        "\n",
        "    print(\"Replacing words with vocabulary numbers...\")\n",
        "    #if type_data == 'train':\n",
        "        #max_tweet_len = max([len(tweet) for tweet in tweets])\n",
        "    #else:\n",
        "        #max_tweet_len = 40 #Empirically obtained :P\n",
        "    max_tweet_len = 20\n",
        "    numbered_tweets = []\n",
        "    numbered_labels = []\n",
        "    for tweet_num, (tweet, label) in enumerate(zip(tweets, labels)):\n",
        "        current_tweet = []\n",
        "\n",
        "        for word in tweet:\n",
        "            if word in vocab.token2id:\n",
        "                current_tweet.append(vocab.token2id[word] + 1)\n",
        "\n",
        "        if len(current_tweet) <= max_tweet_len:\n",
        "            current_tweet_len = len(current_tweet)\n",
        "            for i in range(max_tweet_len - current_tweet_len):\n",
        "                current_tweet.append(0)\n",
        "            numbered_tweets.append(current_tweet)\n",
        "            numbered_labels.append(label)\n",
        "\n",
        "        else:\n",
        "            while len(current_tweet) > max_tweet_len:\n",
        "                numbered_tweets.append(current_tweet[:max_tweet_len])\n",
        "                numbered_labels.append(label)\n",
        "                current_tweet = current_tweet[max_tweet_len:]\n",
        "            if len(current_tweet) > 1:\n",
        "                current_tweet_len = len(current_tweet)\n",
        "                for i in range(max_tweet_len - current_tweet_len):\n",
        "                    current_tweet.append(0)\n",
        "                numbered_tweets.append(current_tweet)\n",
        "                numbered_labels.append(label)\n",
        "\n",
        "    print(\"Replaced words with vocabulary numbers\")\n",
        "    del tweets\n",
        "    labels = np.array(numbered_labels)\n",
        "    del numbered_labels\n",
        "    return (numbered_tweets, labels, len(vocab))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f3dz0nK-I3W"
      },
      "source": [
        "# Создание и обучение модели нейронной сети. Один слой LSTM, с сигмоидной функцией и слоем дропаут, оптимизация adam\n",
        "def create_nn(vocab_len=None, max_tweet_len=None):\n",
        "    if vocab_len == None:\n",
        "        print(\"Error: Vocabulary not initialized\")\n",
        "        return\n",
        "    if max_tweet_len == None:\n",
        "        print(\"Error: Please specify max tweet length\")\n",
        "        return\n",
        "\n",
        "    nn_model = Sequential()\n",
        "    nn_model.add(Embedding(input_dim=(vocab_len + 1), output_dim=32,\n",
        "                           mask_zero=True))\n",
        "    nn_model.add(LSTM(128))\n",
        "    nn_model.add(Dense(32, activation='sigmoid', kernel_regularizer=l2(0.05)))\n",
        "    nn_model.add(Dropout(0.3))\n",
        "    nn_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    nn_model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=[\n",
        "                     'accuracy'])\n",
        "\n",
        "    print(\"Created neural network model\")\n",
        "    return nn_model\n",
        "\n",
        "def get_nn(vocab_len=None, max_tweet_len=None):\n",
        "    if 'model_nn.hdf5' in os.listdir('.'):\n",
        "        # response = raw_input('Neural network model found. Do you want to load'\\\n",
        "        #                     ' it? (Y/n): ')\n",
        "        response = input('Neural network model found. Do you want to load it? (Y/n): ')\n",
        "        if response.lower() in ['n', 'no', 'nah', 'nono', 'nahi', 'nein']:\n",
        "            return create_nn(vocab_len, max_tweet_len)\n",
        "        else:\n",
        "            print(\"Loading model...\")\n",
        "            nn_model = load_model('model_nn.hdf5')\n",
        "            print(\"Loaded model\")\n",
        "            return nn_model\n",
        "    else:\n",
        "        return create_nn(vocab_len, max_tweet_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBe_C6uiGr9L"
      },
      "source": [
        "# Основная функци обучения модели\n",
        "def get_data_and_nn(tweets=None, labels=None, nn_model=None):\n",
        "    if tweets is None and labels is None:\n",
        "        tweets, labels, vocab_len = init_with_vocab()\n",
        "    elif tweets is not None and labels is not None:\n",
        "        pass\n",
        "    else:\n",
        "        print(\"One of tweets or labels given, but not the other\")\n",
        "        return\n",
        "    if not nn_model:\n",
        "        max_tweet_len = max([len(tweet) for tweet in tweets])\n",
        "        nn_model = get_nn(vocab_len, max_tweet_len)\n",
        "    return tweets, labels, nn_model\n",
        "\n",
        "def train_nn(tweets=None, labels=None, nn_model=None):\n",
        "    tweets, labels, nn_model = get_data_and_nn(tweets, labels, nn_model)\n",
        "\n",
        "    # Callbacks (extra features)\n",
        "    tb_callback = TensorBoard(log_dir='./Tensorboard/' + str(time.time()))\n",
        "    early_stop = EarlyStopping(monitor='loss', min_delta=0.025, patience=6)\n",
        "    lr_reducer = ReduceLROnPlateau(monitor='loss', factor=0.5, min_lr=0.00001,\n",
        "                                patience=2, min_delta=0.1) # epsilon=0.1) # WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
        "    # saver = ModelCheckpoint('model_nn.h5', monitor='val_acc')\n",
        "    saver = ModelCheckpoint('model_nn.hdf5', monitor='val_acc')\n",
        "\n",
        "    try:\n",
        "        # nn_model.fit(tweets, labels, epochs=50, batch_size=8192, callbacks= # `validation_split` is only supported for Tensors or NumPy arrays\n",
        "        #             [tb_callback, early_stop, lr_reducer, saver], \n",
        "        #             validation_split=0.2)\n",
        "        nparray_tweets = np.array(tweets)\n",
        "        nparray_labels = np.array(labels)\n",
        "        nn_model.fit(nparray_tweets, nparray_labels, epochs=50, batch_size=8192, callbacks=\n",
        "                    [tb_callback, early_stop, lr_reducer, saver], \n",
        "                    validation_split=0.2, verbose=1)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    # nn_model.save('model_nn.h5')\n",
        "    nn_model.save('model_nn.hdf5')\n",
        "    print(\"Saved model: model_nn.hdf5\")\n",
        "    del tweets\n",
        "    del labels\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG7Cv18uGw-v",
        "outputId": "f330d4fd-3f40-4b36-fa5f-4e37a13f419d"
      },
      "source": [
        "# Cоздаем вокабуляр и обучаем модель \n",
        "tweets, labels, nn_model = get_data_and_nn()\n",
        "print(tweets[:2])\n",
        "print(labels[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data...\n",
            "First 2 tweets\n",
            " [['0', '1467810369', 'Mon Apr 06 22:19:45 PDT 2009', 'NO_QUERY', '_TheSpecialOne_', \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"], ['0', '1467810672', 'Mon Apr 06 22:19:49 PDT 2009', 'NO_QUERY', 'scotthamilton', \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\"]]\n",
            "Preprocessing data...\n",
            "100,000 tweet(s) pre-processed out of 1,600,000\n",
            "200,000 tweet(s) pre-processed out of 1,600,000\n",
            "300,000 tweet(s) pre-processed out of 1,600,000\n",
            "400,000 tweet(s) pre-processed out of 1,600,000\n",
            "500,000 tweet(s) pre-processed out of 1,600,000\n",
            "600,000 tweet(s) pre-processed out of 1,600,000\n",
            "700,000 tweet(s) pre-processed out of 1,600,000\n",
            "800,000 tweet(s) pre-processed out of 1,600,000\n",
            "900,000 tweet(s) pre-processed out of 1,600,000\n",
            "1,000,000 tweet(s) pre-processed out of 1,600,000\n",
            "1,100,000 tweet(s) pre-processed out of 1,600,000\n",
            "1,200,000 tweet(s) pre-processed out of 1,600,000\n",
            "1,300,000 tweet(s) pre-processed out of 1,600,000\n",
            "1,400,000 tweet(s) pre-processed out of 1,600,000\n",
            "1,500,000 tweet(s) pre-processed out of 1,600,000\n",
            "1,600,000 tweet(s) pre-processed out of 1,600,000\n",
            "\n",
            "Cleaning data...\n",
            "Vocabulary found. Do you want to load it? (Y/n):y\n",
            "Loading vocabulary...\n",
            "Loaded vocabulary\n",
            "Replacing words with vocabulary numbers...\n",
            "Replaced words with vocabulary numbers\n",
            "Created neural network model\n",
            "[[419, 36, 1605, 7550, 399, 286, 1675, 407, 1667, 813, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [280, 1063, 1017, 33, 308, 38, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "[1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23VX6EgjjaEf",
        "outputId": "c37eeedb-4e0d-4313-f410-d7fca3906582"
      },
      "source": [
        "train_nn(tweets, labels, nn_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "155/155 [==============================] - 25s 159ms/step - loss: 0.4619 - accuracy: 0.8062 - val_loss: 0.4985 - val_accuracy: 0.7772\n",
            "Epoch 2/50\n",
            "155/155 [==============================] - 24s 153ms/step - loss: 0.4618 - accuracy: 0.8060 - val_loss: 0.4990 - val_accuracy: 0.7773\n",
            "Epoch 3/50\n",
            "155/155 [==============================] - 24s 152ms/step - loss: 0.4616 - accuracy: 0.8062 - val_loss: 0.4988 - val_accuracy: 0.7770\n",
            "Epoch 4/50\n",
            "155/155 [==============================] - 23s 151ms/step - loss: 0.4615 - accuracy: 0.8064 - val_loss: 0.4989 - val_accuracy: 0.7771\n",
            "Epoch 5/50\n",
            "155/155 [==============================] - 24s 152ms/step - loss: 0.4617 - accuracy: 0.8059 - val_loss: 0.4991 - val_accuracy: 0.7771\n",
            "Epoch 6/50\n",
            "155/155 [==============================] - 24s 152ms/step - loss: 0.4611 - accuracy: 0.8063 - val_loss: 0.4992 - val_accuracy: 0.7770\n",
            "Epoch 7/50\n",
            "155/155 [==============================] - 24s 152ms/step - loss: 0.4609 - accuracy: 0.8068 - val_loss: 0.4989 - val_accuracy: 0.7770\n",
            "Saved model: model_nn.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV2gHfyh4NP8",
        "outputId": "7cb8b017-a026-4cdd-a811-703b1b547ad9"
      },
      "source": [
        "# Оцениваем качество модели\n",
        "tweets_test, labels_test, _ = init_with_vocab(type_data='test')\n",
        "print(nn_model.evaluate(np.array(tweets_test), np.array(labels_test), batch_size=32))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data...\n",
            "[['0', '2329087315', 'Thu Jun 25 10:20:06 PDT 2009', 'NO_QUERY', 'mephistolesnc', \"@Nakialjackson Aww...that's sad \"], ['0', '2329087373', 'Thu Jun 25 10:20:07 PDT 2009', 'NO_QUERY', 'boo_kay', '@vmprfreak '], ['0', '2329087698', 'Thu Jun 25 10:20:07 PDT 2009', 'NO_QUERY', 'swgalibertarian', 'Talking to a GAGOV candidate in about an hour. Time to figure out exactly what questions I want to ask him... SOOO many, not enough time! ']]\n",
            "First 2 tweets fron 595\n",
            " [\"@Nakialjackson Aww...that's sad \", '@vmprfreak '] [0.0, 0.0]\n",
            "Preprocessing data...\n",
            "\n",
            "Cleaning data...\n",
            "[list(['aww', 'sad']) list([])] \n",
            " ['kid', 'amp'] \n",
            " [0.0 0.0] \n",
            " [0.0, 0.0]\n",
            "Loading vocabulary...\n",
            "Loaded vocabulary\n",
            "Replacing words with vocabulary numbers...\n",
            "Replaced words with vocabulary numbers\n",
            "19/19 [==============================] - 0s 3ms/step - loss: 0.4067 - accuracy: 0.8239\n",
            "[0.4067313075065613, 0.8239316344261169]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}