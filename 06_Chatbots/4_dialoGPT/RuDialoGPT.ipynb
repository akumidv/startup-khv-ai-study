{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RuDialoGPT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmsPaMmPbr48"
      },
      "source": [
        "# Описание\n",
        "Источник: https://github.com/Grossmend/DialoGPT\n",
        "\n",
        "Статья на хабре https://habr.com/ru/company/icl_services/blog/548244/\n",
        "\n",
        "\n",
        "Модель доступна на Hugging Face Model Hub https://huggingface.co/Grossmend/rudialogpt3_medium_based_on_gpt2 . Также можете скачать с Google Drive https://drive.google.com/file/d/1FCwByq-VkeW1_cje4sIB5KaY0S9a8u8b/view?usp=sharing.\n",
        "\n",
        "Описание как использовать https://huggingface.co/Grossmend/rudialogpt3_medium_based_on_gpt2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KmHwJqptSnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b246efbc-c9a9-4c47-98d6-219dcb8549cf"
      },
      "source": [
        "#!pip install torch==1.6.0 torchvision==0.7.0 # cudatoolkit=10.2 # torch==1.8.1 (current)\n",
        "!pip install transformers==4.4.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.4.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 46.3MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 49.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.2) (4.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.2) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.2) (1.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.2) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.2) (3.7.4.3)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jVW9Ad_s-mk",
        "outputId": "c2e74274-18e4-4ba7-d0b8-2a7183c3065f"
      },
      "source": [
        "import os\n",
        "path_model='/content/RuDialoGPT/model'\n",
        "if not os.path.exists(path_model):\n",
        "  if not os.path.exists(\"/content/RuDialoGPT.zip\"):\n",
        "    !gdown --id 1FCwByq-VkeW1_cje4sIB5KaY0S9a8u8b\n",
        "  !unzip /content/RuDialoGPT.zip\n",
        "\n",
        "# Проверяем, что все на месте\n",
        "if not os.path.isdir(path_model):\n",
        "  raise Exception(f\"Path '{path_model}' not found!\")\n",
        "if not os.path.isfile(os.path.join(path_model, 'config.json')):\n",
        "  raise Exception(f\"On path '{path_model}' file 'config.json' not found!\")\n",
        "if not os.path.isfile(os.path.join(path_model, 'pytorch_model.bin')):\n",
        "  raise Exception(f\"On path '{path_model}' file 'pytorch_model.bin' not found!\")\n",
        "if not os.path.isfile(os.path.join(path_model, 'vocab.json')):\n",
        "  raise Exception(f\"On path '{path_model}' file 'vocab.json' not found!\")\n",
        "if not os.path.isfile(os.path.join(path_model, 'tokenizer_config.json')):\n",
        "  raise Exception(f\"On path '{path_model}' file 'tokenizer_config.json' not found!\")\n",
        "if not os.path.isfile(os.path.join(path_model, 'special_tokens_map.json')):\n",
        "  raise Exception(f\"On path '{path_model}' file 'special_tokens_map.json' not found!\")\n",
        "if not os.path.isfile(os.path.join(path_model, 'merges.txt')):\n",
        "  raise Exception(f\"On path '{path_model}' file 'merges.txt' not found!\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FCwByq-VkeW1_cje4sIB5KaY0S9a8u8b\n",
            "To: /content/RuDialoGPT.zip\n",
            "1.32GB [00:16, 81.3MB/s]\n",
            "Archive:  /content/RuDialoGPT.zip\n",
            "   creating: RuDialoGPT/\n",
            "  inflating: RuDialoGPT/talk_with_model.ipynb  \n",
            "   creating: RuDialoGPT/model/\n",
            "  inflating: RuDialoGPT/model/tokenizer_config.json  \n",
            "  inflating: RuDialoGPT/model/config.json  \n",
            "  inflating: RuDialoGPT/model/pytorch_model.bin  \n",
            "  inflating: RuDialoGPT/model/merges.txt  \n",
            "  inflating: RuDialoGPT/model/special_tokens_map.json  \n",
            "  inflating: RuDialoGPT/model/vocab.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkJBZ0tDhuOx",
        "outputId": "da6dbcf6-d7e9-4b59-f218-135666bce45a"
      },
      "source": [
        "# Клонируем модель (но не скачиваем сами файлы модели)\n",
        "!git clone https://huggingface.co/Grossmend/rudialogpt3_medium_based_on_gpt2\n",
        "!ls -l rudialogpt3_medium_based_on_gpt2/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'rudialogpt3_medium_based_on_gpt2'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 36 (delta 14), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (36/36), done.\n",
            "total 2944\n",
            "-rw-r--r-- 1 root root     827 Jun  4 22:32 config.json\n",
            "-rw-r--r-- 1 root root     135 Jun  4 22:32 flax_model.msgpack\n",
            "-rw-r--r-- 1 root root 1270925 Jun  4 22:32 merges.txt\n",
            "-rw-r--r-- 1 root root     135 Jun  4 22:32 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root    1891 Jun  4 22:32 README.md\n",
            "-rw-r--r-- 1 root root     275 Jun  4 22:32 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root     549 Jun  4 22:32 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 1713123 Jun  4 22:32 vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b4TeIw7ypL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afa0645-e708-408f-928a-8e71ede641ff"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_model)\n",
        "model = AutoModelForCausalLM.from_pretrained(path_model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-jnz4lxZApe"
      },
      "source": [
        "# Функция определения длины диалога\n",
        "def get_length_param(text: str, tokenizer) -> str:\n",
        "    tokens_count = len(tokenizer.encode(text))\n",
        "    if tokens_count <= 15:\n",
        "        len_param = '1'\n",
        "    elif tokens_count <= 50:\n",
        "        len_param = '2'\n",
        "    elif tokens_count <= 256:\n",
        "        len_param = '3'\n",
        "    else:\n",
        "        len_param = '-'\n",
        "    return len_param"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBzg18D_ZGt7",
        "outputId": "24c0d112-b74d-4bcd-9809-9c60c859f838"
      },
      "source": [
        "# Тестирование 5ть циклов на параметрах по умолчанию\n",
        "for step in range(5):\n",
        "    \n",
        "    input_user = input(\"===> User:\")\n",
        "    \n",
        "    # encode the new user input, add parameters and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(f\"|0|{get_length_param(input_user)}|\" + input_user + tokenizer.eos_token +  \"|1|1|\", return_tensors=\"pt\")\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "    \n",
        "    # generated a response\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        num_return_sequences=1,\n",
        "        max_length=512,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        temperature = 0.6,\n",
        "        mask_token_id=tokenizer.mask_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        unk_token_id=tokenizer.unk_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        device='cuda', #device='cpu',\n",
        "    )\n",
        "    \n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(f\"===> RuDialoGPT: {tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===> User:Привет!\n",
            "===> RuDialoGPT: И тебе привет!\n",
            "===> User:Вежливый бот?\n",
            "===> RuDialoGPT: Напоминает\n",
            "===> User:Что напоминает?\n",
            "===> RuDialoGPT: Бот\n",
            "===> User:Бот напоминает бота? Это тавтологоия\n",
            "===> RuDialoGPT: Это что-то вроде \"ты мне не друг, приятель\"\n",
            "===> User:Умный?\n",
            "===> RuDialoGPT: Я не знаю, что это.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6bV4MVVcKdr"
      },
      "source": [
        "#@title Параметры модели\n",
        "\n",
        "# Поле 'Max length tokens generate' должно быть больше 8, но меньше 256\n",
        "max_length = 256 #@param {type:\"integer\"}\n",
        "# def 3 Поле 'No repeat ngram size' должно быть больше или равно 1, но меньше или равно 10.\n",
        "no_repeat_ngram_size = 3 #@param {type:\"integer\"}\n",
        "# def 100 Поле 'Top K' должно быть больше или равно 1, но меньше или равно 500. Текущее значение: \n",
        "top_k = 100 #@param {type:\"integer\"}\n",
        "# def 0.9 Поле 'Top P' должно быть больше или равно 0.01, но меньше или равно 1.0.\n",
        "top_p = 0.9 #@param {type:\"number\"}\n",
        "# def 0.6 Поле 'Temperature' должно быть больше или равно 0.01, но меньше или равно 1.0. \n",
        "temperature = 0.6 #@param {type:\"number\"}\n",
        "# def 5. Поле 'Num responses return' должно быть больше или равно 1, но меньше или равно 10. \n",
        "num_return = 5 #@param {type:\"integer\"}\n",
        "# def True\n",
        "do_sample = True  #@param {type:\"boolean\"}\n",
        "# def True\n",
        "is_always_use_length = True  #@param {type:\"boolean\"}\n",
        "# Поле 'Length generate' должно принимать одно из следующих значений: [0, 1, 2, 3].\n",
        "length_gen = 1  #@param {type:\"integer\"}\n",
        "# def False. debug\n",
        "log_debug = True  #@param {type:\"boolean\"}\n",
        "\n",
        "params = { \n",
        "        \"max_length\": max_length,\n",
        "        \"no_repeat_ngram_size\": no_repeat_ngram_size, \n",
        "        \"top_k\": top_k,                              \n",
        "        \"top_p\": top_p,                              \n",
        "        \"temperature\": temperature,                   \n",
        "        \"num_return_sequences\": num_return,           \n",
        "        \"do_sample\": do_sample,\n",
        "        \"device\": 'cuda', #'cuda:0', # 'cpu',\n",
        "        \"is_always_use_length\": is_always_use_length,\n",
        "        \"length_generate\": length_gen, \n",
        "        \"log_debug\": log_debug\n",
        "    }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br8EoLQ1eacP"
      },
      "source": [
        "# Генерация ответа\n",
        "def generate(full_inputs, params): # inputs = [{'speaker': 0, 'text': 'Привет, как день прошел?'},{'speaker': 1, 'text': 'Хорошо, а у тебя как?'}]\n",
        "  if len(full_inputs) > 5: # Ограничиваем кол-во истории в диалоге на 2 предыдущих + вопрос\n",
        "    inputs = full_inputs[-5:]\n",
        "  else:\n",
        "    inputs = full_inputs\n",
        "  inputs_text = ''\n",
        "  for input_ in inputs:\n",
        "    if params['is_always_use_length']:\n",
        "        length_param = get_length_param(input_['text'], tokenizer)\n",
        "    else:\n",
        "        length_param = '-'\n",
        "    inputs_text += f\"|{input_['speaker']}|{length_param}|{input_['text']}\"\n",
        "  inputs_text += f\"|1|{params['length_generate']}|\"\n",
        "\n",
        "  if params['log_debug']:\n",
        "    print(f\"        [debug] Затравка вопроса для модели: {inputs_text}\")\n",
        "  inputs_token_ids = tokenizer.encode(inputs_text, return_tensors='pt')\n",
        "\n",
        "  try:\n",
        "    # ToDo make this asynchronous\n",
        "    outputs_token_ids = model.generate(\n",
        "        inputs_token_ids,\n",
        "        max_length=params['max_length'],\n",
        "        no_repeat_ngram_size=params['no_repeat_ngram_size'],\n",
        "        do_sample=params['do_sample'],\n",
        "        top_k=params['top_k'],\n",
        "        top_p=params['top_p'],\n",
        "        temperature=params['temperature'],\n",
        "        num_return_sequences=params['num_return_sequences'],\n",
        "        device=params['device'],\n",
        "        mask_token_id=tokenizer.mask_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        unk_token_id=tokenizer.unk_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "  except Exception as e:\n",
        "    print(f\"===> Error generate: {str(e)}\")\n",
        "    return {'inputs': '', 'outputs': '', 'status': False, 'msg': f\"{str(e)}\"}\n",
        "\n",
        "  outputs = [tokenizer.decode(x, skip_special_tokens=True) for x in outputs_token_ids]\n",
        "  outputs = [x.split('|')[-1] for x in outputs]\n",
        "\n",
        "  return {'inputs': inputs, 'outputs': outputs, 'status': True, 'msg': ''}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "E26-gmE9ek20",
        "outputId": "01db5818-90eb-4100-cf9c-e4f09722d21c"
      },
      "source": [
        "# Диалог с чатоботом\n",
        "print('Введите quit для остановки')\n",
        "if params['log_debug']:\n",
        "  print(f\"\\n===> debug Params generate: {params}\\n\\n\")\n",
        "inputs = []\n",
        "while True:\n",
        "  user_input = input(\"USER:\")\n",
        "  if user_input == \"quit\":\n",
        "    \"stop talking!\"\n",
        "    break\n",
        "  inputs.append({'speaker': 0, 'text': user_input})\n",
        "  response_data = generate(inputs, params)\n",
        "  bot_resposnse = response_data['outputs'][0]\n",
        "  variants_responses = response_data['outputs']\n",
        "  \n",
        "  if not response_data['status']:\n",
        "    print(\"BOT (ошибка):\", response_data['msg'])\n",
        "  else:\n",
        "    bot_resposnse = response_data['outputs'][0]\n",
        "    inputs.append({'speaker': 1, 'text': bot_resposnse})\n",
        "    variants_responses = response_data['outputs']\n",
        "    print(\"\\n>>>BOT:\", bot_resposnse)\n",
        "    if params['log_debug']:\n",
        "     print(\"        [debug] other variants ===> :\", variants_responses[1:])\n",
        "     "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Введите quit для остановки\n",
            "\n",
            "===> debug Params generate: {'max_length': 256, 'no_repeat_ngram_size': 3, 'top_k': 100, 'top_p': 0.9, 'temperature': 0.6, 'num_return_sequences': 5, 'do_sample': True, 'device': 'cuda', 'is_always_use_length': True, 'length_generate': 1, 'log_debug': True}\n",
            "\n",
            "\n",
            "USER:Привет!\n",
            "        [debug] Затравка вопроса для модели: |0|1|Привет!|1|1|\n",
            "\n",
            ">>>BOT: И тебе привет, соседушка\n",
            "        [debug] other variants ===> : ['Вай, спасибо большое', 'Братан, привет', 'И тебе привет)', 'Здравствуй!']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-350a437ecabe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"USER:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"stop talking!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}